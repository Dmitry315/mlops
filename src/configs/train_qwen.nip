trainer: !HFQwenTrainer
  qwen_params:
    hidden_size: 1024
    head_dim: 128
    intermediate_size: 3072
    num_hidden_layers: 28
    max_window_layers: 28
    num_attention_heads: 16
    num_key_value_heads: 8
    max_position_embeddings: 2048
    attention_dropout: 0.0
    hidden_act: "silu"
    attention_bias: False
  tokenizer_path: "src/experiments/bpe_greek_tokenizer_v1"
  trainer_config_params:
    output_dir: "src/experiments/results"
    num_train_epochs: 1
    max_steps: -1
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 16
    optim: "adamw_torch"
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_epsilon: 1e-8
    save_steps: 25
    save_total_limit: 1
    logging_steps: 10
    learning_rate: 1e-5
    weight_decay: 1e-6
    fp16: True
    max_grad_norm: 1.0
    warmup_ratio: 0.03
    group_by_length: False
    lr_scheduler_type: "cosine"
    eval_strategy: "no"
    max_length: 2048
    resume_from_checkpoint: &checkpoint False
    completion_only_loss: False
    assistant_only_loss: False
    seed: 42
  train_dataset: !get_pretrain_data
    data_path: "src/experiments/data/fineweb2/train.jsonl"
  save_path: "src/experiments/models/qwen_pretrain"
  add_size_to_name: True
  resume_from_checkpoint: *checkpoint