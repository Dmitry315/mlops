trainer: !HFBPETokenizerTrainer
  bpe_init_params:
    dropout: 0
  bpe_trainer_params:
    vocab_size: 150000
    min_frequency: 2
    special_tokens: ["<|bos|>", "<|unk|>", "<|sep|>", "<|pad|>", "<|eos|>", "<|system|>", "<|user|>", "<|assistant|>"]
  train_corpus_files:
    - "src/experiments/data/fineweb2/all.txt"
  save_path:
    local_path: "src/experiments/bpe_greek_tokenizer_v1/"
  tokenizer_fast_params:
    padding_side: "left"
    truncation_side: "left"
    bos_token: "<|bos|>"
    eos_token: "<|eos|>"
    unk_token: "<|unk|>"
    sep_token: "<|sep|>"
    pad_token: "<|pad|>"
    additional_special_tokens: ["<|system|>", "<|user|>", "<|assistant|>"]